<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>The Variational Autoencoder Part I</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      .smallcaps{font-variant: small-caps;}
      .line-block{white-space: pre-line;}
      .column{display: inline-block;}
  </style>
  <link rel="stylesheet" href="writeups.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header>
<h1 class="title">The Variational Autoencoder Part I</h1>
</header>
<nav id="TOC">
<ul>
<li><a href="#variational-bayes">Variational Bayes</a><ul>
<li><a href="#problem-setup">Problem Setup</a></li>
<li><a href="#variational-inference">Variational Inference</a></li>
</ul></li>
<li><a href="#variational-autoencoders">Variational Autoencoders</a><ul>
<li><a href="#density-estimation-and-the-elbo">Density Estimation and the ELBO</a></li>
<li><a href="#the-road-towards-gradient-descent">The Road Towards Gradient Descent</a></li>
<li><a href="#reparametrization-trick">Reparametrization Trick</a></li>
<li><a href="#where-neural-nets-tie-in">Where Neural Nets Tie In</a></li>
</ul></li>
<li><a href="#recommended-reading">Recommended Reading</a></li>
</ul>
</nav>
<h2 id="variational-bayes">Variational Bayes</h2>
<h3 id="problem-setup">Problem Setup</h3>
<p>Suppose we have a task in which we are given some data <span class="math inline">\(x\)</span> and are looking to model things with some latent variables <span class="math inline">\(Z\)</span> which govern our data. We will represent this probabilistically with a very simple model: <span class="math display">\[
p(x, z) = p(x | z) p(z)
\]</span> Here, <span class="math inline">\(p(z)\)</span> is a prior on <span class="math inline">\(z\)</span> and <span class="math inline">\(p(x | z)\)</span> is a conditional likelihood. We do not have access to the marginal <span class="math inline">\(p(x)\)</span> (wouldn’t that be nice?).</p>
<p>The generative process looks like</p>
<ol type="1">
<li>Sample some latent variables <span class="math inline">\(z\)</span> using <span class="math inline">\(p(z)\)</span></li>
<li>Use the values of <span class="math inline">\(z\)</span> to draw from <span class="math inline">\(p(x | z)\)</span> to get <span class="math inline">\(x\)</span>.</li>
</ol>
<p>Given these initial conditions, we would like to calculate the posterior <span class="math inline">\(p(z | x)\)</span>. We know from Bayes rule that this is <span class="math display">\[
p(z | x) = \frac{p(x, z) \times p (z)} {p(x)}.
\]</span></p>
<p>So why can’t we use compute this? Oftentimes, computing <span class="math inline">\(p(x)\)</span> turns out to be intractable. Here, <span class="math inline">\(p(x)\)</span> is called the <em>evidence</em>. If <span class="math inline">\(Z\)</span> is discrete, we must compute <span class="math display">\[
\sum_{support(Z)} p(x | z) p(z).
\]</span> As the size of <span class="math inline">\(z\)</span> increases, the difficulty of this sum grows exponentially. For example, if we have <span class="math inline">\(Z\)</span> be Bernoulli, we must compute <span class="math inline">\(2^n\)</span> sums if <span class="math inline">\(z\)</span> is of dimension <span class="math inline">\(n\)</span>. Similarly, if <span class="math inline">\(Z\)</span> is continuous, we may have issues computing <span class="math display">\[
\int p(x | z) p(z) dz.
\]</span> Having accepted that we can’t compute the posterior directly, we turn to other methods to approximate the posterior. One way of doing so is with MCMC, where we build a Markov chain whose stationary distribution represents our final outcome.</p>
<figure align="center">
<img width=900 src="https://theclevermachine.files.wordpress.com/2012/10/metropolis2.gif">
<figcaption>
Visualization of Metropolis sampler. The red line is the current proposal distribution, while the black line is the target distribution. From Dustin Stansbury’s website.
</figcaption>
</figure>
<p>MCMC, however, is a beast to get going. I’ll talk about the Metropolis/Metropolis Hastings algorithm, since that is not only the most well known MCMC algorithm, but sadly also the only one I know. (The popular Gibbs sampling is a special form of Metropolis-Hastings.) Like the accept/reject method of sampling, Metropolis-Hastings requires the choice of proposal distributions. However, here, the choice of proposal is much more finicky then accept/reject. If the variance of the proposal is not large enough, we might get highly autocorrelated samples. If it is too small, we might get many rejections in our sampling. The chain may never converge in reasonable time. And so on. Properly utilizing this method is an art in itself. And even MCMC suffers from issues of scalability eventually.</p>
<h3 id="variational-inference">Variational Inference</h3>
<p>So, now we want a new method of getting at the posterior. We now turn to variational inference. Variational inference involves approximation of the posterior through using members of a new family of distributions <span class="math inline">\(\mathcal Q\)</span> for which computation is much easier to evaluate (e.g. is tractable). We refer to <span class="math inline">\(\mathcal Q\)</span> as the <em>variational family</em>. We want to find <span class="math inline">\(q^*(z | x) \in \mathcal Q\)</span> which is most similar to the true posterior <span class="math inline">\(p(z | x)\)</span>. For now, we will measure this similarity/dissimilarity by using the Kullback-Leibler divergence, although other measures are certainly possible.</p>
<p>I will use <span class="math inline">\(D\)</span> to represent the Kullback-Leibler divergence. With our current formulation, this means we want <span class="math inline">\(q^*(z | x)\)</span> such that <span class="math display">\[
q^*(z | x) =
\text{argmin}_{q(z | x) \in \mathcal Q} D \big (q(z|x), p(z|x) \big ).
\]</span> There is a reason that the assymmetric KL-divergence is given in this order, but that’s a discussion for another time. Our optimization problem is intrinsically linked to a bound on the “loglihood” <span class="math inline">\(\text{log } p(x)\)</span> known as the <em>variational lower bound</em> or the <em>Evidence Lower BOund (ELBO)</em>: for any <span class="math inline">\(q \in \mathcal Q\)</span>, we have <span class="math display">\[
\text{log } p(x) = \text{log } \int \frac{q(z|x)}{q(z|x)} p(x,z) dz \geq
\int q(z|x)\text{ log }\frac{p(x,z)}{q(z|x)} dz
= \text{E}_{q(z|x)} \text{ log }\frac{p(x,z)}{q(z|x)}.
\]</span> Here, Jensen’s inequality has been applied to get the inequality in the middle. You may have seen this bound before in the derivation of the EM algorithm.</p>
<p>Now, a simple manipulation of this bound using the identity <span class="math inline">\(p(x, z) = p(z | x) p(x)\)</span> shows that <span class="math display">\[
\text{E}_{q(z|x)} \text{ log }\frac{p(x,z)}{q(z|x)}
 = \text{E}_{q(z|x)} \text{ log }{p(x)} +
\text{E}_{q(z|x)} \text{ log }\frac{p(z|x)}{q(z|x)} \\
= \text{log }p(x) - D \big (q(z|x), p(z|x) \big ).
\]</span> Note that we are able to remove the conditional expectation from <span class="math inline">\(\text{log } p(x)\)</span> as it does not depend on <span class="math inline">\(z\)</span>. Our final expression shows that with <span class="math inline">\(p(x)\)</span> held constant, maximizing the ELBO is equivalent to minimizing the same KL divergence <span class="math inline">\(D(q(z|x), p(z|x))\)</span>. Thus, variational inference is typically expressed in terms of maximizing the ELBO.</p>
<h2 id="variational-autoencoders">Variational Autoencoders</h2>
<h3 id="density-estimation-and-the-elbo">Density Estimation and the ELBO</h3>
<p>Up until now, we have been thinking in terms of posterior inference, the traditional setting for variational Bayes. We are now going to take this one step further. Let’s say we want to maximize the log-likelihood term <span class="math inline">\(\text{log } p(x)\)</span>. We do not necessarily know <span class="math inline">\(p\)</span> in advance, so that this becomes a density estimation problem. Direct density estimation problems are difficult, so what we are going to do is instead maximize the ELBO in an attempt to maximize <span class="math inline">\(\text{log } p(x)\)</span>. Let’s reexamine the ELBO <span class="math display">\[
\text{E}_{q(z|x)} \text{ log }\frac{p(x,z)}{q(z|x)}.
\]</span> Because the marginal <span class="math inline">\(p(x, z)\)</span> decomposes into <span class="math inline">\(p(x|z) p(z)\)</span>, we can rewrite the ELBO as <span class="math display">\[
\text{E}_{q(z|x)} \text{ log }\frac{p(x|z)p(z)}{q(z|x)}.
\]</span> We can attach a natural interpretation to the term <span class="math inline">\(p(x|z)\)</span>. This conditional probability can be thought of as being part of the generative part of the model: it explains what to do when given <span class="math inline">\(x\)</span> in the generative process. Moreover, as we have explained in the previous section, the term <span class="math inline">\(q(z|x)\)</span> is used to perform inference on the posterior <span class="math inline">\(p(z|x)\)</span>. The variational autoencoder is a technique based around optimizing the ELBO over difference choices of <span class="math inline">\(q(z|x)\)</span> and <span class="math inline">\(p(z|x)\)</span>. That is, our objective is to find <span class="math display">\[
\text{max}_{p(x|z) \in \mathcal P, \ q(z|x) \in \mathcal Q}
\text{E}_{q(z|x)} \text{ log }\frac{p(x|z)p(z)}{q(z|x)}.
\]</span></p>
<h3 id="the-road-towards-gradient-descent">The Road Towards Gradient Descent</h3>
<p>We are going to maximize the ELBO by using a combination of Monte-Carlo integration and gradient descent, a technique known as <em>Black Box Variational Inference</em>. Accordingly, let’s begin to place the problem in a parametric setting. We will let <span class="math inline">\(p(x|z) \in \mathcal P\)</span> by indexed by <span class="math inline">\(\theta\)</span> in <span class="math inline">\(\Theta\)</span>, and let <span class="math inline">\(q(x|z) \in \mathcal Q\)</span> be indexed by <span class="math inline">\(\lambda \in \Lambda\)</span>. Our problem is now to find <span class="math display">\[
\text{max}_{\theta \in \Theta, \lambda \in \Lambda}
\text{E}_{q_\lambda (z|x)} \text{ log }\frac{p_\theta(x|z)p(z)}{q_\lambda(z|x)}.
\]</span> We will also assume some regularity of the ELBO in <span class="math inline">\((\theta, \lambda)\)</span>, so that we can compute gradients and interchange integrals/expectations and derivatives. To perform gradient descent, we of course need to be able to compute the gradient <span class="math display">\[
\nabla_{\theta, \lambda} \ \big [
\text{E}_{q_\lambda (z|x)} \text{ log }\frac{p_\theta(x|z)p(z)}{q_\lambda(z|x)} \big ].
\]</span> However, it turns out that this not so easy to compute. Let’s split this into two parts: <span class="math display">\[
\nabla_{\theta, \lambda} \ \big [
\text{E}_{q_\lambda (z|x)} \text{ log }\frac{p_\theta(x|z)p(z)}{q_\lambda(z|x)} \big ]
= \nabla_{\theta, \lambda} \ \big [
\text{E}_{q_\lambda (z|x)} \text{ log }p_\theta(x|z)p(z) \big ]
- \nabla_{\theta, \lambda} \ \big [
\text{E}_{q_\lambda (z|x)} \text{ log } q_\lambda(z|x) ) \big ].
\]</span> With very mild regularity assumptions (for Leibniz’s rule), we can just push the gradient into the expectation for the first part: <span class="math display">\[
\nabla_{\theta, \lambda} \ \big [
\text{E}_{q_\lambda (z|x)} \text{ log }p_\theta(x|z)p(z) \big ] =
\text{E}_{q_\lambda (z|x)} \big [ \nabla_{\theta}
\text{ log }p_\theta(x|z)p(z) \big ]
\]</span> and then this term can be approximated via Monte-Carlo integration. It’s the second term that gives us trouble. The quantity <span class="math display">\[
\nabla_{\theta, \lambda} \ \big [
\text{E}_{q_\lambda (z|x)} \text{ log } q_\lambda(z|x) ) \big ]
\]</span> has the expectation taken depending on <span class="math inline">\(\lambda\)</span>, so that we can’t push the gradient inside the expectation.</p>
<p>A way we can attempt to get this around this is with a so-called “score function estimator”, named after the same score function used in Fisher information. By evaluating the gradient term <span class="math display">\[\nabla_\lambda \text{ log } q_\lambda(z|x)
= \frac{\nabla_\lambda q_\lambda(z|x)}{q_\lambda(z|x)}
\]</span> one can show that <span class="math display">\[
\nabla_{\theta, \lambda} \ \big [
\text{E}_{q_\lambda (z|x)} \text{ log }p_\theta(x|z)p(z) \big ]
= \text{E}_{q_\lambda (z|x)} \big [ \big ( \text{ log }p_\theta(x|z)p(z)
- \text{log } q(z|x) \big ) \nabla_\lambda q(z|x) \big ]
\]</span> which we can now use Monte Carlo integration to take estimates of. We are going spare the reader the full derivation of this identity because as it turns out, the Monte Carlo estimates of this quantity have been shown to have very high variance, too high to reasonably use in many cases. It seems that we are back at square one with the issue of computing the gradient.</p>
<h3 id="reparametrization-trick">Reparametrization Trick</h3>
<p>We need to get an reasonable estimate of the gradient of the ELBO. To do so, we rely on a small trick known as the <em>reparametrization trick</em>. We attempt to represent the family of distributions <span class="math inline">\(\mathcal Q\)</span> in terms of a deterministic transformation <span class="math inline">\(T_\lambda(\epsilon, x)\)</span> of a fixed distribution <span class="math inline">\(\epsilon\)</span>. This sounds more complicated than is really is. For example, if <span class="math inline">\(\mathcal Q\)</span> is the family of <span class="math inline">\(N(x, \sigma)\)</span> distributions, we can let <span class="math inline">\(\epsilon\)</span> be a standard <span class="math inline">\(N(0,1)\)</span> distribution. If <span class="math inline">\(q_{\sigma^*} (z|x^*)\)</span> is distributed as <span class="math inline">\(N(x^*, \sigma^*)\)</span>, we write the distribution in terms of <span class="math inline">\(\epsilon\)</span> as <span class="math inline">\(x^* + \sigma^\star \epsilon\)</span>. Many families of probability distributions (such as location-scale families) can be represented this way.</p>
<p>Why do we do this? Recall that earlier we could not push in the gradient to compute <span class="math display">\[
\nabla_{\theta, \lambda} \ \big [
\text{E}_{q_\lambda (z|x)} \text{ log } q_\lambda(z|x) ) \big ]
\]</span> because the expectation was taken against <span class="math inline">\(q_\lambda(z|x)\)</span>. But with the reparametrization trick, we can now do <span class="math display">\[
\nabla_{\theta, \lambda} \ \big [
\text{E}_{q_\lambda (z|x)} \text{ log } q_\lambda(z|x) ) \big ]
= \nabla_{\theta, \lambda} \ \big [
\text{E}_{p(\epsilon)} \text{ log }
q_\lambda \big ( T_\lambda(\epsilon, x) \big ) \big ]
= \text{E}_{p(\epsilon)} \big [ \nabla_\lambda
\text{ log } q_\lambda \big ( T_\lambda(\epsilon, x) \big ) \big ].
\]</span> Now, we can take Monte-Carlo estimates of this quantity. It turns out that the Monte-Carlo estimates of the gradient expressed as <span class="math display">\[
\text{E}_{q_\lambda (z|x)} \big [ \nabla_{\theta}
\text{ log }p_\theta(x|z)p(z) \big ]
+ \text{E}_{p(\epsilon)} \big [ \nabla_\lambda
\text{ log } q_\lambda \big ( T_\lambda(\epsilon, x) \big ) \big ].
\]</span> have much lower variance than the score function estimator, allows us to reasonably perform gradient descent. Finally, the hard part is out of the way! With this being done, we can now optimize the ELBO through standard gradient descent methods.</p>
<h3 id="where-neural-nets-tie-in">Where Neural Nets Tie In</h3>
<p>Let’s go back a little bit and recall that our overall goal was to find <span class="math display">\[
\text{max}_{\theta \in \Theta, \lambda \in \Lambda}
\text{E}_{q_\lambda (z|x)} \text{ log }\frac{p_\theta(x|z)p(z)}{q_\lambda(z|x)}.
\]</span> We remarked earlier that <span class="math inline">\(p_\theta(x|z)\)</span> can be thought of as related to the generative part of the model and learning <span class="math inline">\(q_\theta(z|x)\)</span> can be thought of as performing inference. Both of these quantities also have natural interpretations related to a certain type of architecture for neural networks.</p>
<figure align="center">
<img width=500 src="https://blog.keras.io/img/ae/autoencoder_schema.jpg">
<figcaption>
Simple diagram of autoencoder. Taken from the Keras blog.
</figcaption>
</figure>
<p>In the standard autoencoder architecture, we use a neural network to predict it’s own inputs. The goal is to learn a latent represention. The <em>encoder</em> part of the network maps the original input to the latent representation, while the <em>decoder</em> part attempts to reconstruct the original input using the latent representation. Training is done to minimize some form of reconstruction loss.</p>
<p>This is surprisingly similar to our variational inference task. In particular, we can think <span class="math inline">\(q_\lambda (z|x)\)</span> as being a “stochastic encoder”, <span class="math inline">\(Z\)</span> as a stochastic encoding, and <span class="math inline">\(p_\theta (x|z)\)</span> as a “stochastic decoder”. We can exploit this analogy by parametrizing <span class="math inline">\(p_\theta(x|z)\)</span> and <span class="math inline">\(q_\lambda (z|x)\)</span> using layers of a neural network. That is, the weights in the encoding layer of a neural network will form the parameters <span class="math inline">\(\theta\)</span>, and the weights in decoding layers will contain the parameters <span class="math inline">\(\lambda\)</span>.</p>
<figure align="center">
<img width=500 src="https://jaan.io/images/encoder-decoder.png">
<figcaption>
Diagram of variational autoencoder. Taken from the Jaan Altosaar’s blog. Note that I use different symbols for the parameters.
</figcaption>
</figure>
<p>For example, we might choose <span class="math inline">\(p_\theta(x|z)\)</span> and <span class="math inline">\(q_\lambda(z|x)\)</span> to be both Gaussian (as often is the case), and parametrize them as <span class="math display">\[
q_\lambda (z|x) \sim N(\mu = Enc_1 (\lambda, z), \ \Sigma = Enc_2(\lambda, z) \odot I) \\
p_\theta (x|z) \sim N(\mu = Dec_1 (\theta, z), \ \Sigma = Dec_2(\theta, z) \odot I)
\]</span> A typical prior on <span class="math inline">\(z\)</span> might simply be a standard multivariate normal <span class="math inline">\(p(z) \sim N(0, I)\)</span>. Here, the point of using neural networks is exploit their power as functional appromixators, so that we can learn complex configurations in the parameter space for the distributions.</p>
<p>For the variational autoencoder with our current setup, the overall process can be thought of like this:</p>
<ul>
<li>We input some data <span class="math inline">\(x_i\)</span></li>
<li>The encoder layers output some means and covariances for a multivariate normal conditional on <span class="math inline">\(x_i\)</span></li>
<li>We can sample from a <span class="math inline">\(N(0, I)\)</span> random variable and convert them into a sample from <span class="math inline">\(z\)</span> using the output of the encoder layers and the reparametrization trick.</li>
<li>The sample is now run through the decoder to get more means and covariances: this is a “stochastic decoding”.</li>
</ul>
<h2 id="recommended-reading">Recommended Reading</h2>
<ul>
<li><a href="http://ruishu.io/2018/03/14/vae/">Rui Shu’s blog post on density estimation</a></li>
<li><a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/">Jaan Altosaar’s writeup</a></li>
<li><a href="http://approximateinference.org/accepted/HoffmanJohnson2016.pdf">Performing some ELBO surgery</a></li>
<li><a href="https://arxiv.org/pdf/1702.08658.pdf">Towards a deeper understanding</a></li>
</ul>
<p>The last link is quite notable in that the authors are able to alleviate the blurriness problems with VAEs.</p>
</body>
</html>
