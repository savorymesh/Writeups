<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Exponential Random Graph Models</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      .smallcaps{font-variant: small-caps;}
      .line-block{white-space: pre-line;}
      .column{display: inline-block;}
  </style>
  <link rel="stylesheet" href="writeups.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header>
<h1 class="title">Exponential Random Graph Models</h1>
</header>
<nav id="TOC">
<ul>
<li><a href="#brief-remarks">Brief Remarks</a></li>
<li><a href="#sufficiency">Sufficiency</a><ul>
<li><a href="#minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</a></li>
<li><a href="#results-concerning-sufficient-statistics">Results Concerning Sufficient Statistics</a></li>
</ul></li>
<li><a href="#exponential-families-of-distributions">Exponential Families of Distributions</a></li>
</ul>
</nav>
<h2 id="brief-remarks">Brief Remarks</h2>
<p>This is a writeup to help me better understand EGRMs. I am a bit dubious about this method of modelling right now.</p>
<p>To understand the exponential random graph model, I think it’s important to first conduct a review of sufficient statistics as well as exponential families of distributions.</p>
<p>A brief remark about notation: to better illustrate the concept of sufficiency, I will often use the notation <span class="math inline">\(P(X = x \ | \ \theta)\)</span> in lieu of <span class="math inline">\(P_{\theta} (X = x)\)</span>. In general, I have always found probability notation to be quite awkward to work with.</p>
<p>Mathematical correctness obliges me to remark that all discussion is done excluding non-measurable functions, etc.</p>
<h2 id="sufficiency">Sufficiency</h2>
<p>We’ll first discuss the concept of sufficiency, a fundamental concept in the theory of statistics. At a high level, sufficient statistics are those which are able to summarize all relevant information held in a sample regarding specific parameters. ‘Good’ estimators are typically functions of sufficient statistics. Indeed, estimators which don’t depend on sufficient statistics can often be improved through conditioning on sufficient statistics, a process known as <em>Rao-Blackwellizing an estimator</em>.</p>
<p>Let <span class="math inline">\(X = (X_1, X_2, \ldots, X_n)\)</span> be an i.i.d. sample from a probability distribution parameterized by finite-dimensional <span class="math inline">\(\theta\)</span>. We say the statistic <span class="math inline">\(T( X ) = \big (T_1 (X), \ T_2 (X), \ \ldots, T_k (X) \big )\)</span> is <em>sufficient</em> for <span class="math inline">\(\theta\)</span> if conditioning on <span class="math inline">\(T\)</span> removes the dependency of <span class="math inline">\(X\)</span> on <span class="math inline">\(\theta\)</span>. That is <span class="math display">\[
P( X =  x \ | \ T = t, \ \theta)
= P( X = x \ | \ T = t)
\]</span> is valid for all <span class="math inline">\(x\)</span> and <span class="math inline">\(t\)</span>. This is typically given the interpretation that the sufficient statistic ‘captures’ all the information about <span class="math inline">\(\theta\)</span> in the sample. To illustrate this concept, we’ll do an obligatory example with the Bernoulli distribution.</p>
<p>Suppose <span class="math inline">\(X\)</span> is drawn from a Bernoulli(<span class="math inline">\(p\)</span>) distribution. Let’s show that <span class="math inline">\(T = \sum_{i=1}^n X_i\)</span> is sufficient for <span class="math inline">\(p\)</span>. Of course, <span class="math inline">\(T \sim\)</span> Binomial(<span class="math inline">\(n, \ p\)</span>). In general, when proving suffiency, we can note that <span class="math display">\[
P( X =  x \ | \ T=t, \ \theta)
= \frac{P( X =  x, \ T = t \ | \ \theta)}{P(T = t \ | \ \theta)}
= \frac{P( X =  x \ | \ \theta)}{P(T = t \ | \theta)}
\]</span> since the event <span class="math inline">\(\{T = t\} \subseteq \{X = x\}\)</span>. So, for our case, we have <span class="math display">\[
P( X =  x \ | \ T=t, \ p)
= \frac{\prod_{i=1}^n p^{x_i} (1-p)^{1 - x_i}}{\binom{n}{t} p^t(1-p)^{n-t}}
= \frac{p^{\sum_{i=1}^n x_i} (1-p)^{n - \sum_{i=1}^n x_i}} {\binom{n}{t} p^t(1-p)^{n-t}}
\\
= \frac{p^t(1-p)^t}{\binom{n}{t} p^t(1-p)^{n-t}}
= \frac{1}{\binom{n}{t}}.
\]</span></p>
<p>Since the final expression does not contain <span class="math inline">\(p\)</span>, we have that <span class="math inline">\(T\)</span> is sufficient for <span class="math inline">\(p\)</span>. NB that while the sample <span class="math inline">\(X\)</span> is of arbitrary dimension <span class="math inline">\(n\)</span>, this shows the information it contains about <span class="math inline">\(p\)</span> can be captured in a single value <span class="math inline">\(T(X)\)</span>. Moreover, it can be shown that injective functions of sufficient statistics are themselves sufficient statistics. It follows then that the natural estimator of <span class="math inline">\(p\)</span>, the sample mean <span class="math inline">\(\bar X = \frac{T}{n}\)</span> is also sufficient for <span class="math inline">\(p\)</span>.</p>
<p>While it was not too much work to prove sufficiency in the previous example, proving sufficiency using the definition can be daunting for more complicated estimation problems. The <em>Fisher-Neyman factorization theorem</em> simplifies this task by providing an alternative method to characterize sufficiency: if the distribution for <span class="math inline">\(X\)</span> has density <span class="math inline">\(f(x \ | \ \theta)\)</span>, we have that <span class="math inline">\(T(X)\)</span> is sufficient if and only if we can find <span class="math inline">\(h(x)\)</span> and <span class="math inline">\(g(T(x), \theta)\)</span> such that <span class="math inline">\(f(x \ | \ \theta) = g(T(x), \theta) h(x)\)</span>. In other words, a statistic is sufficient if and only we can factor the joint density of the sample into two parts : one that depends on the parameters and on <span class="math inline">\(x\)</span> only through the statistic <span class="math inline">\(T(X)\)</span> and the other that can more freely depend on the data <span class="math inline">\(x\)</span>. For example, with our simple example with the Bernoulli distribution, we could alternatively prove that <span class="math inline">\(T = \sum_{i=1}^n X_i\)</span> is sufficient by noting <span class="math display">\[
f(x \ | \ \theta) = P(X = x \ | \ \theta)
= \prod_{i=1}^n p^{x_i} (1-p)^{1 - x_i}
\\
= p^{\sum_{i=1}^n x_i} (1-p)^{n - \sum_{i=1}^n x_i}
= p^{t} (1-p)^{n - t} \times 1
\]</span> so that we have <span class="math inline">\(g(T(x), p) = p^{t} (1-p)^{n - t}\)</span> and <span class="math inline">\(h(x) = 1\)</span>.</p>
<p>As an additional note, we remark that sufficient statistics always exist as, amusingly, we can consider the sample <span class="math inline">\(X\)</span> itself as a statistic.</p>
<h3 id="minimal-sufficiency-and-completeness">Minimal Sufficiency and Completeness</h3>
<p>Sufficiency is inherently tied to the concept of data reduction. As remarked before, sufficient statistics capture all the information the sample contains for estimation of appropriate parameters. As there can be multiple sufficient statistics for given parameters, we naturally would like to find one that summarizes the data as efficiently as possible. This brings us to the concept of minimal sufficient statistics. A <em>minimal sufficient statistic</em> for the parameter(s) <span class="math inline">\(\theta\)</span> is a sufficient statistic that is a function of every other sufficient statistic for <span class="math inline">\(\theta\)</span>. Because functions can only map values in their domain to one value in the codomain, but a value in the codomain may have many values mapped to it, minimal sufficient statistics provide the most data reduction among sufficient statistics. Outside of rare pathological cases, minimal sufficient statistics always exist outside. Any injective function of a minimal sufficient statistic is another minimal sufficient statistic. Modulo injective mappings, however, minimal sufficient statistics are unique.</p>
<p>Sufficient statistics seem to be quite distinguished statistics. Historically, statisticians tried to prove several results about sufficient statistics. However, they found they needed an additional condition to hold to complete the proofs. This might be the reason behind the name for <em>complete statistics</em>. (Just kidding). We say a statistic <span class="math inline">\(T(X)\)</span> is <em>complete</em> if for every real-valued function <span class="math inline">\(g\)</span> we have that for all values of <span class="math inline">\(\theta\)</span> <span class="math display">\[
E_\theta(g(T)) = 0 \implies P_\theta (g(T)) = 0.
\]</span> Completeness is a notoriously confusing property to interpret on first glance. While there are some belabored attempts to interpret completeness such as <a href="http://www2.math.ou.edu/~cremling/teaching/lecturenotes/stat/ln5.pdf">this one</a>, it is often simply regarded as being an extra technical condition to check to obtain results for sufficient statistics. If a minimal sufficient statistic exists, any complete sufficient statistic is automatically minimal sufficient for <span class="math inline">\(\theta\)</span>.</p>
<h3 id="results-concerning-sufficient-statistics">Results Concerning Sufficient Statistics</h3>
<p>In the beginning of this section, we remarked that ‘good’ estimators are typically functions of sufficient statistics. We will clarify that statement in this subsection. Our first result is probably the easiest to see yet also one of the most important: the maximum likelihood estimator of a parameter must be a function of a sufficient statistics.</p>
<ul>
<li>UMVUEs link to sufficient statistics</li>
</ul>
<h2 id="exponential-families-of-distributions">Exponential Families of Distributions</h2>
</body>
</html>
